#!/opt/local/bin/python3.2

import argparse
import csv
import datetime
import os
import re
import shutil
import sys
import xml.etree.cElementTree as ET

import rr.common

class nyrr:
    def __init__(self,args):
        """
        day:  range of days in which to look for results
        month:  month in which to look for results
        year:  year in which to look for results
        memb_list:  membership list
        race_list:  file containing list of races
        """
        self.day = args.day
        self.month = args.month
        self.year = args.year
        if args.day is not None: 
            self.start_date = datetime.date(int(self.year), 
                    int(self.month), int(self.day[0])) 
            self.stop_date = datetime.date(int(self.year), 
                    int(self.month), int(self.day[1]))
        self.memb_list = args.membership_list
        self.race_list = args.race_list
        self.output_file = args.output_file
        self.verbose = args.verbose

        # We seem to need this for all element searches now.
        # Actually we do not.  But keep it around anyway.
        self.xmlns = 'http://www.w3.org/1999/xhtml'


        # Need to remember the current URL.
        self.downloaded_url = None


        # Parse the membership list.
        mlreader = csv.reader(open(args.membership_list,'r'))
        first_name = []
        first_name_regex = []
        last_name = []
        last_name_regex = []
        for row in mlreader:
            fname = row[0]
            lname = row[1]
            first_name.append(fname)
            first_name_regex.append(re.compile(fname,re.IGNORECASE))
            last_name.append(lname)
            last_name_regex.append(re.compile(lname,re.IGNORECASE))

        self.first_name = first_name
        self.last_name = last_name
        self.first_name_regex = first_name_regex
        self.last_name_regex = last_name_regex

        self.compile_results();
        self.tidy(self.output_file)



    def compile_results(self):
        """
        Either download the requested results or go through the
        provided list.
        """

        self.initialize_output_file()
        if self.race_list is None:
            self.compile_web_results()
        else:
            self.compile_local_results()



    def initialize_output_file(self):
        """
        Construct an HTML skeleton.
        """
        ofile = ET.Element('html')

        head = ET.SubElement(ofile,'head')

        link = ET.SubElement(head,'link')
        link.set('rel','stylesheet')
        link.set('href','rr.css')
        link.set('type','text/css')

        body = ET.SubElement(ofile,'body')

        ET.ElementTree(ofile).write(self.output_file)

        rr.common.pretty_print_xml(self.output_file)


    def compile_web_results(self):
        """
        Download the requested results and compile them.
        """
        self.download_master_file() 
        self.process_master_file()


    def construct_state_match_pattern(self,state):
        """
        Want to match strings like

        http://www.coolrunning.com/results/07/ma/Jan16_Coloni_set1.shtml

        So we construct a regular expression to match against
        all the dates in the specified range.
        """

        #pattern = 'http://ww.coolrunning.com/results/'
        pattern = '/results/'
        pattern += self.start_date.strftime('%y')
        pattern += '/'
        pattern += state
        pattern += '/'
        pattern += self.start_date.strftime('%b')

        # continue with a regexp to match any of the days in the range.
        day_range = '('
        for day in range(self.start_date.day,self.stop_date.day):
            day_range += "%d_|" % day
        day_range += '%d_)' % self.stop_date.day

        pattern += day_range

        pattern += '.*shtml'
        self.log('Match pattern is %s' % pattern,2) 
        r = re.compile(pattern,re.DOTALL)
        return(r)


    def log(self,msg,level):
        """
        Prints a message depending on verbosity level.
        """
        if level <= self.verbose:
            print('%s%s' % ('\t'*level,msg))


    def process_master_file(self):
        """
        Compile results for the specified state.
        We assume that we have the state file stored locally.
        """

        tree = ET.parse('index.html')
        root = tree.getroot()
        root = self.remove_namespace(root)

        # Date match pattern 
        date_pattern = '%s' % self.start_date.strftime('%y')

        # Need to find the list of results.
        # body/div/div/dvi/div/div/table
        pattern = './/body/table/tr/td/table/tr/td/table/tr/td/table/tr/td/a'
        anodes = root.findall(pattern)
        for anode in anodes:
            href = anode.get('href')

            # Do not follow any result not internalized here.
            # That means it must have a cgi-bin part.
            if not re.search('cgi-bin',href):
                continue

            # The last entry goes into the archive.  Don't 
            # go there.
            if re.search('resultsarchive',href):
                continue


            # Look for the date
            #a href="http://web2.nyrrc.org/cgi-bin/start.cgi/
            #  aes-programs/results/startup.html?
            #  result.id=b11211&amp;result.year=2011">
            year_part = href.split('?')[1].split('&')[1].split('=')[1]
            if int(year_part) != self.start_date.year:
                continue

            date_part = href.split('?')[1].split('&')[0].split('=')[1]
            month_part = date_part[2:4]
            if month_part != self.start_date.strftime('%m'):
                continue
                   
            day_part = date_part[4:]
            day_expr = '(%02d' % self.start_date.day
            for day in range(self.start_date.day+1,self.stop_date.day+1):
                day_expr += '|%02d' % day
            day_expr += ')'
            if not re.match(day_expr,day_part):
                continue
                   
            self.process_individual_race(anode)


    def process_individual_race(self,anode):
        """
        """
        self.log(anode.text,1)
        rr.common.download_file(anode.get('href'), 'nyrr.html')
        self.tidy('nyrr.html')
        
        tree = ET.parse('nyrr.html') 
        root = tree.getroot() 
        root = self.remove_namespace(root)

        # body/table/tr/td/form
        pattern = './/body/table/tr/td/form'
        forms = root.findall(pattern)
        if len(forms) == 0:
            return
        form = forms[0]
        action = form.get('action')
        self.log('action is %s' % action,2)
        

        post_params = {}
        post_params['team_code'] = 'RARI'
        post_params['input.lname'] = ''
        post_params['input.fname'] = ''
        post_params['input.bib']= ''
        post_params['search.method'] = 'search.team'
        post_params['overalltype'] = 'All'
        post_params['input.agegroup.m'] = '12 to 14'
        post_params['input.agegroup.f'] = '12 to 14'
        post_params['teamgender'] = ''
        post_params['team_code']= 'RARI'
        post_params['items.display'] = '500'
        post_params['AESTIVACVNLIST'] = 'overalltype,input.agegroup.m,input.agegroup.f,teamgender,team_code'
        params = urllib.parse.urlencode(post_params)

        rr.common.download_file(action, 'nyrr3.html')
        #fp = urllib.request.urlopen(action,params.encode('utf-8'))
        #x = fp.read()
        #fp2 = open('nyrr3.html','wb')
        #fp2.write(x);
        #fp2.close()
        fp.close()

        self.tidy('nyrr3.html')

        tree = ET.parse('nyrr3.html') 
        root = tree.getroot() 
        root = self.remove_namespace(root)


        tables = root.findall('.//body/table')
        if len(tables) < 3:
            return

        div = ET.Element('div')
        hr = ET.Element('hr')
        hr.set('class','race_header')
        div.append(hr)

        # Get the title.
        # head/title 
        h1 = ET.Element('h1')
        node = tree.findall('.//head/title')
        h1.text = node[0].text
        div.append(h1)

        p = ET.Element('p')
        span = ET.Element('span')
        span.text = 'Full results can be found at '
        p.append(span)
        a = ET.Element('a')
        a.set('href','http://www.nyrr.org')
        a.text = 'NYRR.'
        p.append(a)
        div.append(p)

        # Get the metadata.
        # body/table/tr/td/p
        p = root.findall('.//body/table/tr/td/p')
        div.append(p[0])

        # Get the results.
        # body/table/tr/
        node = ET.Element('table')
        table = tables[2]
        trs = table.getchildren()
        if len(trs) < 2:
            return

        header_row = self.construct_header_row(trs[0])
        node.append(header_row)

        for tr in trs[1:]:
            node.append(tr)
        div.append(node)

        self.insert_common_div(div)


    def construct_header_row(self,tr):
        """
        Need to process the header row.
        """
        output_tr = ET.Element('tr')
        output_tr.set('bgcolor','#EEEEEE')
        tds = tr.getchildren()
        for td in tds:
            anode = td.findall('.//a')
            if len(anode) > 0:
                other_td = ET.Element('td')
                other_td.text = anode[0].text
                output_tr.append(other_td)
            else:
                output_tr.append(td)


        return(output_tr)




    def compile_race_results_xml_02(self,race_file):
        """
        """
        pattern = './/body/table/tr/td/table/tr/td/table/tr/td/div/pre'

        tree = ET.parse(race_file)
        root = tree.getroot()
        self.remove_namespace(root)
        nodes = root.findall(pattern)
        text = nodes[0].text

        r = []
        for line in text.split('\n'):
            if self.match_against_membership(line):
                r.append(line)

        if len(r) > 0:
            self.insert_race_results(r,race_file)


    def is_xml_pattern_02(self,race_file):
        """
        """
        pattern = './/body/table/tr/td/table/tr/td/table/tr/td/div/pre'

        tree = ET.parse(race_file)
        root = tree.getroot()
        self.remove_namespace(root)
        nodes = root.findall(pattern)
        if len(nodes) > 0:
            return True
        else:
            return False


    def is_xml_pattern_01(self,race_file):
        """
        It would seem that Cape Cod Road Runners use this XML 
        pattern.
        """
        pattern = './/body/table/tr/td/table/tr/td/table/tr/td/div/table/tr'

        tree = ET.parse(race_file)
        root = tree.getroot()
        self.remove_namespace(root)
        nodes = root.findall(pattern)
        if len(nodes) > 0:
            return True
        else:
            return False


    def compile_race_results_xml_01(self,race_file):
        """
        This is the format generally used by Cape Cod
        Road Runners.
        """
        pattern = './/body/table/tr/td/table/tr/td/table/tr/td/div/table/tr'

        tree = ET.parse(race_file)
        root = tree.getroot()
        self.remove_namespace(root)

        trs = root.findall(pattern)

        results = []
        for tr in trs:
            tds = tr.getchildren()

            runner_name = tds[2].text 
            for idx in range(0,len(self.first_name_regex)): 
                fregex = self.first_name_regex[idx] 
                lregex = self.last_name_regex[idx] 
                if fregex.search(runner_name) and lregex.search(runner_name): 
                    tr = self.remove_namespace(tr)
                    results.append(tr)

        if len(results) > 0:
            # Prepend the header.
            tr = self.remove_namespace(trs[0])
            results.insert(0,tr)
            self.insert_race_results_xml_01(results,race_file)


    def compile_race_results_vanilla_coolrunning(self,race_file):
        """
        Just do a brute-force line-by-line search for names.
        """
        r = []
        for rline in open(race_file):
            line = rline.rstrip()
            if self.match_against_membership(line):
                r.append(line)

        if len(r) > 0:
            self.insert_race_results(r,race_file)



    def compile_race_results(self,race_file):
        """
        Go through a race file and collect results.
        body table tr td table tr td table tr td div TABLE TR
        """
        if self.is_xml_pattern_02(race_file):
            self.log('XML pattern 2', 2)
            self.compile_race_results_xml_02(race_file)
        elif self.is_xml_pattern_01(race_file):
            self.log('XML pattern 1 (weary travelers)', 2)
            try: 
                self.compile_race_results_xml_01(race_file)
            except:
                self.log('Unhandled condition, skipping it...',0)
        else:
            self.log('Vanilla Coolrunning pattern', 2)
            self.compile_race_results_vanilla_coolrunning(race_file)



    def construct_common_div(self,race_file):
        """
        """
        div = ET.Element('div')
        div.set('class','race')
        hr = ET.Element('hr')
        hr.set('class','race_header')
        div.append(hr)

        # The H1 tag has the race name.
        # The H1 tag comes from the only H1 tag in the race file.
        tree = ET.parse(race_file)
        root = tree.getroot()
        pattern = './/{%s}h1' % self.xmlns
        source_h1 = root.findall(pattern)[0]

        h1 = ET.Element('h1')
        h1.text = source_h1.text
        div.append(h1)

        # The first H2 tag has the location and date.
        # The H2 tag comes from the only H2 tag in the race file.
        pattern = './/{%s}h2' % self.xmlns
        source_h2 = root.findall(pattern)[0]

        h2 = ET.Element('h2')
        h2.text = source_h2.text
        div.append(h2)

        # Append the URL if possible.
        if self.downloaded_url is not None:
            text = '<p class="provenance">Complete results <a href="%s">here</a> on CoolRunning.</p>' % self.downloaded_url
            p = ET.XML(text)
            div.append(p)

        return(div)
            


    def insert_race_results_xml_01(self,results,race_file):
        """
        Insert CoolRunning results into the output file.
        """
        div = self.construct_common_div(race_file)

        table = ET.Element('table')
        for tr in results:
            if tr is not None: 
                table.append(tr)

        div.append(table)

        self.insert_common_div(div)



    def insert_race_results(self,result,race_file):
        """
        Insert CoolRunning results into the output file.
        """
        div = self.construct_common_div(race_file)

        pre = ET.Element('pre')
        pre.set('class','actual_results')

        root = ET.parse(race_file).getroot()
        root = self.remove_namespace(root)
        banner = self.parse_banner(root)

        text = '\n'
        for line in result:
            text += line + '\n'

        pre.text = banner + text
        div.append(pre)

        self.insert_common_div(div)




    def insert_common_div(self,div):
        """
        The DIV element has been completely construct, so
        now we just put it into the output file.
        """
        tree = ET.parse(self.output_file)
        root = tree.getroot()
        body = root.findall('.//body')[0]
        body.append(div)

        ET.ElementTree(root).write(self.output_file)


    def parse_banner(self,root):
        """
                   The Andrea Holden 5k Thanksgiving Race            
         PLC    Time  Pace  PLC/Group  PLC/Sex Bib#   Name             
           1   16:40  5:23    1 30-39    1 M   142 Brian Allen        

        """
        pattern = './/pre'
        try: 
            pre = root.findall(pattern)[0]
        except IndexError:
            return('')

        # Stop when we find the first "1"
        banner = ''
        for line in pre.text.split('\n'):
            if re.match('\s+1',line):
                # found it
                break
            else:
                banner += line + '\n'

        return(banner)


    def match_against_membership(self,line):
        """
        """
        #z = zip(self.first_name_regex,self.last_name_regex)
        for idx in range(0,len(self.first_name_regex)):
            fregex = self.first_name_regex[idx]
            lregex = self.last_name_regex[idx]
            if fregex.search(line) and lregex.search(line):
                return(True)
        return(False)

    def download_master_file(self):
        """
        Download results for the specified state.

        """
        self.log('Processing www.nyrr.org...',1)
        url = 'http://web2.nyrrc.org/cgi-bin/htmlos.cgi/aes-programs/results/resultsarchive.htm'
        self.log('Downloading %s.' % url,1)
        rr.common.download_file(url, 'index.html')
        self.tidy('index.html')



    def download_race(self,anchor,pattern):
        """
        """
        href = anchor.get('href')
        if href is None:
            return
        match = pattern.search(href)
        if match is None:
            return
        url = 'http://www.coolrunning.com/%s' % href
        local_file = href.split('/')[-1]
        self.log('Downloading %s...' % local_file,1)
        rr.common.download_file(url, local_file)
        self.downloaded_url = url
        try: 
            self.tidy(local_file)
        except IOError:
            self.log('Encountered an error processing %s, skipping it' %
                    local_file,0)
            local_file = None

        return(local_file)



    def tidy(self,html_file):
        """ 
        Tidy up the HTML.
        """
        fp = open(html_file,'rb')
        b = fp.read()
        text = b.decode('latin')
        fp.close()
        text = text.replace('<form','<div')
        text = text.replace('</form','</div')
        fp = open(html_file,'w')
        fp.write(text)
        fp.close()

        rr.common.tidy(html_file)


    def compile_local_results(self):
        """
        Compile results from list of local files.
        """
        for line in open(self.race_list):
            try: 
                self.log('Processing file %s' % line,2) 
                race_file = line.rstrip() 
                self.tidy(race_file) 
                self.compile_race_results(race_file)
            except IOError:
                self.log('Encountered an error processing %s, skipping it' % line,0) 


    def remove_namespace(self,doc): 
        """Remove namespace in the passed document in place.""" 
        ns = '{%s}' % self.xmlns 
        nsl = len(ns) 
        for elem in doc.getiterator(): 
            if elem.tag.startswith(ns): 
                elem.tag = elem.tag[nsl:]

        return(doc)

if __name__ == '__main__':

    # -ml cannot be used with -d, -m, or -y
    # But -y and -m have defaults.
    the_description='Process Coolrunning race results'
    parser = argparse.ArgumentParser(description=the_description)
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-d', '--day', dest='day', 
            nargs=2, help='day range')
    parser.add_argument('-v', '--verbose', dest='verbose',
            default=1,
            choices=range(0,3),
            type=int,
            help='verbosity level, default is 1')
    parser.add_argument('-m', '--month', dest='month',
            default=datetime.date.today().month,
            choices=range(1,13),
            type=int,
            help='month')
    parser.add_argument('-o', '--output', dest='output_file',
            default='results.html', 
            help='output file, default is results.html') 
    parser.add_argument('-y', '--year', dest='year',
            default=datetime.date.today().year, help='year')
    parser.add_argument('--ml', dest='membership_list',
            help='membership list', required=True)
    group.add_argument('--rl', dest='race_list', 
            help='race list')
    args = parser.parse_args()
    o = nyrr(args)

